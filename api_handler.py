# api_handler.py
import google.generativeai as genai
import google.api_core.exceptions
try: import anthropic
except ImportError: anthropic = None
try: import openai
except ImportError: openai = None
import os
from dotenv import load_dotenv
import tkinter.messagebox as messagebox # ì´ˆê¸° ì„¤ì • ì˜¤ë¥˜ìš©
import traceback
import time # íƒ€ì„ì•„ì›ƒ ê°’ í™•ì¸ìš©

import constants

# --- API ì„¤ì • ---
def configure_gemini_api():
    """Gemini API í‚¤ë¥¼ ë¡œë“œí•˜ê³  í´ë¼ì´ì–¸íŠ¸ ì„¤ì •. ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False."""
    try: load_dotenv(dotenv_path=constants.ENV_FILE, override=True)
    except Exception as e: print(f"WARN: .env íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

    api_key = os.getenv(constants.GOOGLE_API_KEY_ENV)
    if not api_key:
        print(f"â„¹ï¸ Gemini API í‚¤({constants.GOOGLE_API_KEY_ENV}) ì—†ìŒ.")
        return False
    try:
        genai.configure(api_key=api_key)
        print(f"âœ… Google Gemini API ì„¤ì • ì™„ë£Œ.")
        return True
    except Exception as e:
        print(f"âŒ Gemini API ì„¤ì • ì˜¤ë¥˜: {e}")
        return False

def configure_claude_api():
    """Claude API í‚¤ë¥¼ ë¡œë“œí•˜ê³  í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì¤€ë¹„. ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False."""
    try: load_dotenv(dotenv_path=constants.ENV_FILE, override=True)
    except Exception as e: print(f"WARN: .env íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

    api_key = os.getenv(constants.ANTHROPIC_API_KEY_ENV)
    if not api_key:
        print(f"â„¹ï¸ Claude API í‚¤({constants.ANTHROPIC_API_KEY_ENV}) ì—†ìŒ.")
        return False
    if anthropic is None:
        print("âŒ Claude API ì‚¬ìš© ë¶ˆê°€: 'anthropic' ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ. `pip install anthropic`")
        return False
    try:
        # ì‹¤ì œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì€ í˜¸ì¶œ ì‹œì ì— ìˆ˜í–‰
        print(f"âœ… Anthropic Claude API ì„¤ì • ì¤€ë¹„ ì™„ë£Œ.")
        return True
    except Exception as e:
        print(f"âŒ Claude API ì„¤ì • ì˜¤ë¥˜: {e}")
        return False

def configure_gpt_api():
    """OpenAI API í‚¤ë¥¼ ë¡œë“œí•˜ê³  í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì¤€ë¹„. ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False."""
    try: load_dotenv(dotenv_path=constants.ENV_FILE, override=True)
    except Exception as e: print(f"WARN: .env íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

    api_key = os.getenv(constants.OPENAI_API_KEY_ENV)
    if not api_key:
        print(f"â„¹ï¸ OpenAI API í‚¤({constants.OPENAI_API_KEY_ENV}) ì—†ìŒ.")
        return False
    if openai is None:
        print("âŒ OpenAI API ì‚¬ìš© ë¶ˆê°€: 'openai' ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ. `pip install openai`")
        return False
    try:
        # ì‹¤ì œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì€ í˜¸ì¶œ ì‹œì ì— ìˆ˜í–‰
        print(f"âœ… OpenAI GPT API ì„¤ì • ì¤€ë¹„ ì™„ë£Œ.")
        return True
    except Exception as e:
        print(f"âŒ OpenAI API ì„¤ì • ì˜¤ë¥˜: {e}")
        return False

def configure_apis():
    """ëª¨ë“  API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •. ê° API ì„¤ì • ì„±ê³µ ì—¬ë¶€ íŠœí”Œ ë°˜í™˜."""
    gemini_configured = configure_gemini_api()
    claude_configured = configure_claude_api()
    gpt_configured = configure_gpt_api()
    print(f"API ì„¤ì • ê²°ê³¼: Gemini={gemini_configured}, Claude={claude_configured}, GPT={gpt_configured}")
    return gemini_configured, claude_configured, gpt_configured

# --- ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ---
def get_gemini_models():
    """Gemini APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    # Gemini API ì„¤ì • í™•ì¸
    if not os.getenv(constants.GOOGLE_API_KEY_ENV): return []
    try:
        configure_gemini_api() # í˜¸ì¶œ ì „ì— ì¬ì„¤ì • ì‹œë„
        all_models = genai.list_models()
        # generateContent ì§€ì› ëª¨ë¸ í•„í„°ë§
        return sorted([m.name for m in all_models if 'generateContent' in m.supported_generation_methods])
    except Exception as e:
        print(f"âŒ Gemini ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

def get_claude_models():
    """Anthropic APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Claude ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    api_key = os.getenv(constants.ANTHROPIC_API_KEY_ENV)
    if not api_key or anthropic is None: return []

    try:
        # Anthropic í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì—¬ê¸°ì„œ ìƒì„±)
        client = anthropic.Anthropic(api_key=api_key)
        # ClaudeëŠ” ëª¨ë¸ ëª©ë¡ ì§ì ‘ ì œê³µ ì•ˆ í•¨ - ì•Œë ¤ì§„ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (í•„ìš”ì‹œ ì—…ë°ì´íŠ¸)
        # ë˜ëŠ”, íŠ¹ì • ì—”ë“œí¬ì¸íŠ¸ê°€ ìˆë‹¤ë©´ í˜¸ì¶œ (í˜„ì¬ ê³µì‹ SDKì—ëŠ” list models ì—†ìŒ)
        # ì•Œë ¤ì§„ ìµœì‹  ëª¨ë¸ ìœ„ì£¼ë¡œ í•˜ë“œì½”ë”© (Claude 3.5 Sonnet, Opus, Haiku ë“±)
        known_models = [
            "claude-3-5-sonnet-20240620",
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229",
            "claude-3-haiku-20240307",
            # "claude-2.1", # í•„ìš”ì‹œ ì´ì „ ëª¨ë¸ ì¶”ê°€
            # "claude-2.0",
            # "claude-instant-1.2"
        ]
        # TODO: Anthropicì—ì„œ ëª¨ë¸ ëª©ë¡ API ì œê³µ ì‹œ í•´ë‹¹ ë¡œì§ìœ¼ë¡œ ë³€ê²½
        print(f"â„¹ï¸ Claude ëª¨ë¸ ëª©ë¡: ì•Œë ¤ì§„ ëª©ë¡ ë°˜í™˜ ({len(known_models)}ê°œ)")
        return sorted(known_models)

    except Exception as e:
        print(f"âŒ Claude ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì•Œë ¤ì§„ ëª©ë¡ ë°˜í™˜ ì¤‘ ì˜¤ë¥˜): {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []


def get_gpt_models():
    """OpenAI APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ GPT ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    api_key = os.getenv(constants.OPENAI_API_KEY_ENV)
    if not api_key or openai is None: return []

    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = openai.OpenAI(api_key=api_key)
        models = client.models.list()
        # ì±„íŒ… ë° ìµœì‹  ëª¨ë¸ ìœ„ì£¼ í•„í„°ë§ (ì˜ˆ: gpt-4, gpt-3.5)
        gpt_models = [m.id for m in models.data if m.id.startswith(('gpt-4', 'gpt-3.5'))]
        print(f"âœ… OpenAI GPT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({len(gpt_models)}ê°œ)")
        return sorted(gpt_models)
    except Exception as e:
        print(f"âŒ GPT ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

def get_available_models():
    """ëª¨ë“  APIì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    models = {
        constants.API_TYPE_GEMINI: get_gemini_models(),
        constants.API_TYPE_CLAUDE: get_claude_models(),
        constants.API_TYPE_GPT: get_gpt_models()
    }
    return models

# --- í”„ë¡¬í”„íŠ¸ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „) ---
def generate_prompt(novel_settings, chapter_arc_notes, scene_plot, length_option, previous_scene_content=None):
    """ì†Œì„¤ ì„¤ì •, ì±•í„° ë…¸íŠ¸, ì¥ë©´ í”Œë¡¯, ì´ì „ ì¥ë©´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    length_request = f"({length_option})"

    # ì†Œì„¤ ì „ì²´ ì„¤ì • ë¸”ë¡
    novel_setting_key = constants.NOVEL_MAIN_SETTINGS_KEY
    novel_setting_content = novel_settings.get(novel_setting_key, "").strip()
    novel_block = ""
    if novel_setting_content:
        novel_block = f"**[ì†Œì„¤ ì „ì²´ ì„¤ì •: {novel_setting_key}]**\n{novel_setting_content}\n---"

    # ì±•í„° ì•„í¬ ë…¸íŠ¸ ë¸”ë¡
    chapter_notes_key = constants.CHAPTER_ARC_NOTES_KEY
    chapter_notes_content = chapter_arc_notes.get(chapter_notes_key, "").strip()
    chapter_block = ""
    if chapter_notes_content:
        chapter_block = f"**[ì´ë²ˆ ì±•í„° ì•„í¬ ë…¸íŠ¸: {chapter_notes_key}]**\n{chapter_notes_content}\n---"

    # ì‘ì„± ê°€ì´ë“œë¼ì¸
    guidelines = f"""
**[ì‘ì„± ê°€ì´ë“œë¼ì¸]**
*   ìš”ì²­ ë¶„ëŸ‰({length_request})ì— ë§ì¶° ì›¹ì†Œì„¤ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
*   ë‹¤ìŒ ì¥ë©´ì´ ê¶ê¸ˆí•´ì§€ë„ë¡ í¥ë¯¸ë¡œìš´ ì‚¬ê±´ì´ë‚˜ ë³µì„ ì„ í¬í•¨í•´ì£¼ì„¸ìš”."""

    # ì¥ë©´ í”Œë¡¯ ë¸”ë¡
    plot_key = constants.SCENE_PLOT_KEY
    plot_content = scene_plot.strip() if scene_plot else 'ììœ ë¡­ê²Œ ì§„í–‰í•´ì£¼ì„¸ìš”.'
    plot_block = f"**[ì´ë²ˆ ì¥ë©´ í”Œë¡¯ ({plot_key})]**\n{plot_content}"

    # --- ì´ì „ ì¥ë©´ ë‚´ìš© ë¸”ë¡ (ìˆ˜ì •) ---
    prev_content_block = ""
    # previous_scene_contentê°€ ì´ì œ ì—¬ëŸ¬ ì¥ë©´ì˜ ê²°í•©ëœ ë‚´ìš©ì¼ ìˆ˜ ìˆìŒ
    if previous_scene_content and previous_scene_content.strip():
        # ë ˆì´ë¸” ìˆ˜ì •: ì´ì „ ì¥ë©´ í•˜ë‚˜ê°€ ì•„ë‹˜ì„ ëª…ì‹œ
        prev_content_block = f"**[ì´ë²ˆ ì±•í„° ì´ì „ ë‚´ìš©]**\n{previous_scene_content}\n---"
        # instruction ìˆ˜ì •: 'ìœ„ ë‚´ìš©' -> 'ì´ì „ ë‚´ìš©' ë˜ëŠ” 'ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©'
        prompt_instruction = "ì§€ê¸ˆê¹Œì§€ì˜ ë‚´ìš©ì— ì´ì–´ì„œ, ì•„ë˜ ì„¤ì •ê³¼ ìš”ì²­ì— ë”°ë¼ **ë‹¤ìŒ ì¥ë©´**ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”."
        final_section_header = "**[ë‹¤ìŒ ì¥ë©´ ì‹œì‘]**"
    else:
        # ì²« ì¥ë©´ì¼ ê²½ìš°
        prompt_instruction = "ë‹¤ìŒ ì£¼ì–´ì§„ ì†Œì„¤ ì„¤ì •, ì±•í„° ë…¸íŠ¸, ì²« ì¥ë©´ì˜ í”Œë¡¯ì„ ë°”íƒ•ìœ¼ë¡œ í¥ë¯¸ì§„ì§„í•œ ì›¹ì†Œì„¤ì˜ **ì²« ì¥ë©´**ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."
        final_section_header = "**[ì›¹ì†Œì„¤ ì²« ì¥ë©´ ì‹œì‘]**"

    # í”„ë¡¬í”„íŠ¸ ì¡°í•©
    prompt_parts = [
        prev_content_block, # ì´ì „ ë‚´ìš©ì´ ë§¨ ì•ì— ì˜¤ë„ë¡ ìˆœì„œ ìœ ì§€
        prompt_instruction,
        novel_block,
        chapter_block,
        plot_block,
        guidelines,
        # ì²« ì¥ë©´ì´ ì•„ë‹ ë•Œì˜ ê°€ì´ë“œë¼ì¸ ìˆ˜ì •
        '*   ì´ì „ ë‚´ìš©ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë„ë¡ ì‘ì„±í•´ì£¼ì„¸ìš”.' if prev_content_block else '*   ë“±ì¥ì¸ë¬¼ì˜ ë§¤ë ¥ê³¼ ì„¸ê³„ê´€ì˜ íŠ¹ì§•ì´ ì˜ ë“œëŸ¬ë‚˜ë„ë¡ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.',
        final_section_header
    ]
    prompt = "\n\n".join(part for part in prompt_parts if part)

    # print("--- ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ---")
    # print(prompt[:500] + "...")
    # print("----------------------")
    return prompt


# --- API í˜¸ì¶œ (ê³µí†µ ì§„ì…ì  ë° ë¶„ê¸°) ---

def generate_webnovel_scene_api_call(api_type, model_name, prompt, system_prompt, temperature=constants.DEFAULT_TEMPERATURE):
    """API íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ"""
    print(f"API HANDLER: Scene generation request received for API='{api_type}', Model='{model_name}'") # DEBUG
    if api_type == constants.API_TYPE_GEMINI:
        return _generate_with_gemini(model_name, prompt, system_prompt, temperature)
    elif api_type == constants.API_TYPE_CLAUDE:
        return _generate_with_claude(model_name, prompt, system_prompt, temperature)
    elif api_type == constants.API_TYPE_GPT:
        return _generate_with_gpt(model_name, prompt, system_prompt, temperature)
    else:
        msg = f"ì˜¤ë¥˜: ì§€ì›ë˜ì§€ ì•ŠëŠ” API íƒ€ì…: {api_type}"
        print(f"âŒ API HANDLER: {msg}")
        return msg, None # Return error message and None for token_info

def generate_summary_api_call(api_type, model_name, text_to_summarize):
    """API íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ìš”ì•½ í•¨ìˆ˜ í˜¸ì¶œ"""
    print(f"API HANDLER: Summary generation request received for API='{api_type}', Model='{model_name}'") # DEBUG
    if not text_to_summarize or not text_to_summarize.strip():
        print("â„¹ï¸ API HANDLER: ìš”ì•½í•  ë‚´ìš© ì—†ìŒ. ë¹ˆ ìš”ì•½ ë°˜í™˜.")
        return "", {constants.INPUT_TOKEN_KEY: 0, constants.OUTPUT_TOKEN_KEY: 0} # Return empty string and zero tokens

    summary_system_prompt = "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì›¹ì†Œì„¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´ì „ ì¤„ê±°ë¦¬ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ëŠ” AIì…ë‹ˆë‹¤. í•µì‹¬ ì‚¬ê±´ê³¼ ì¸ë¬¼ ê´€ê³„ ë³€í™”ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."
    summary_user_prompt = f"""ë‹¤ìŒ ì›¹ì†Œì„¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´ì „ ì¤„ê±°ë¦¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:

{text_to_summarize}

---
**[ìš”ì•½ ê²°ê³¼]**"""

    # Ensure a valid model name is provided for the given API type
    if not model_name:
        msg = f"ì˜¤ë¥˜: '{api_type}' APIì— ëŒ€í•œ ìš”ì•½ ëª¨ë¸ ì´ë¦„ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        print(f"âŒ API HANDLER: {msg}")
        return msg, None

    if api_type == constants.API_TYPE_GEMINI:
        return _generate_with_gemini(model_name, summary_user_prompt, summary_system_prompt, constants.SUMMARY_TEMPERATURE)
    elif api_type == constants.API_TYPE_CLAUDE:
        return _generate_with_claude(model_name, summary_user_prompt, summary_system_prompt, constants.SUMMARY_TEMPERATURE)
    elif api_type == constants.API_TYPE_GPT:
        return _generate_with_gpt(model_name, summary_user_prompt, summary_system_prompt, constants.SUMMARY_TEMPERATURE)
    else:
        msg = f"ì˜¤ë¥˜: ì§€ì›ë˜ì§€ ì•ŠëŠ” API íƒ€ì… (ìš”ì•½): {api_type}"
        print(f"âŒ API HANDLER: {msg}")
        return msg, None # Return error message and None for token_info


# --- APIë³„ ì‹¤ì œ í˜¸ì¶œ í•¨ìˆ˜ ---

def _generate_with_gemini(model_name, prompt, system_prompt, temperature):
    """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
    print(f"API HANDLER (Gemini): Calling model '{model_name}'...") # DEBUG
    token_info = {constants.INPUT_TOKEN_KEY: 0, constants.OUTPUT_TOKEN_KEY: 0}
    error_message = None
    response = None

    try:
        # Gemini API ì„¤ì • í™•ì¸ ë° ì¬ì„¤ì • (Ensure configure is called if needed)
        if not os.getenv(constants.GOOGLE_API_KEY_ENV):
             return "ì˜¤ë¥˜: Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", token_info # Return error, token_info
        # Consider calling configure_gemini_api() here if needed, but it might be redundant if checked at startup
        # genai.configure(api_key=os.getenv(constants.GOOGLE_API_KEY_ENV)) # Safer to reconfigure if unsure

        model_kwargs = {"model_name": model_name}
        # Gemini supports system_instruction directly in GenerativeModel constructor
        if system_prompt and system_prompt.strip():
            model_kwargs["system_instruction"] = system_prompt.strip()

        try:
            temp_value = float(temperature); temp_value = max(0.0, min(2.0, temp_value))
        except (ValueError, TypeError): temp_value = constants.DEFAULT_TEMPERATURE

        model = genai.GenerativeModel(**model_kwargs)
        generation_config = genai.types.GenerationConfig(temperature=temp_value)
        # Define request_timeout (consider making it configurable)
        request_timeout = 720 # Example: 12 minutes

        start_time = time.time()
        response = model.generate_content(
            prompt,
            generation_config=generation_config,
            safety_settings=constants.SAFETY_SETTINGS, # Define SAFETY_SETTINGS in constants if needed, otherwise remove
            request_options={'timeout': request_timeout}
        )
        end_time = time.time()
        print(f"âœ… API HANDLER (Gemini): Response received ({end_time - start_time:.2f}s)")

        # --- Gemini Token Extraction ---
        try:
            if response and hasattr(response, 'usage_metadata'):
                usage_meta = response.usage_metadata
                token_info = {
                    constants.INPUT_TOKEN_KEY: getattr(usage_meta, 'prompt_token_count', 0),
                    constants.OUTPUT_TOKEN_KEY: getattr(usage_meta, 'candidates_token_count', 0) # Note: Gemini uses candidates_token_count
                }
                print(f"ğŸ“Š Gemini Tokens: Input={token_info[constants.INPUT_TOKEN_KEY]}, Output={token_info[constants.OUTPUT_TOKEN_KEY]}")
            elif response and hasattr(response, 'usage'): # Fallback for potential older API versions or different response structures
                 token_info = {
                     constants.INPUT_TOKEN_KEY: getattr(response.usage, 'prompt_tokens', 0),
                     constants.OUTPUT_TOKEN_KEY: getattr(response.usage, 'completion_tokens', 0) # Check if this key exists
                 }
                 print(f"ğŸ“Š Gemini Tokens (Fallback): Input={token_info[constants.INPUT_TOKEN_KEY]}, Output={token_info[constants.OUTPUT_TOKEN_KEY]}")
            else:
                print("âš ï¸ Gemini Tokens: No usage metadata found in response.")
        except Exception as token_err:
            print(f"âš ï¸ Gemini Tokens: Error extracting token info: {token_err}")

        # --- Gemini Response Handling ---
        generated_text = None
        finish_reason_val = "UNKNOWN"
        block_reason = None

        if response and response.prompt_feedback:
             block_reason_enum = getattr(response.prompt_feedback, 'block_reason', None)
             if block_reason_enum:
                  block_reason = block_reason_enum.name if hasattr(block_reason_enum, 'name') else str(block_reason_enum)
                  print(f"âŒ Gemini prompt blocked: {block_reason}")
                  error_message = f"ì˜¤ë¥˜ ë°œìƒ: ì…ë ¥ ë‚´ìš©ì´ ì•ˆì „ ì •ì±…ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (ì‚¬ìœ : {block_reason}).\ní”„ë¡¬í”„íŠ¸ë‚˜ ì„¤ì •ì„ ìˆ˜ì •í•´ë³´ì„¸ìš”."
                  return error_message, token_info # Return error early

        if response and response.candidates:
            # Check for safety ratings within candidates as well
            candidate = response.candidates[0]
            finish_reason_enum = getattr(candidate, 'finish_reason', None)
            finish_reason_val = finish_reason_enum.name if hasattr(finish_reason_enum, 'name') else str(finish_reason_enum)
            print(f"â„¹ï¸ Gemini Finish Reason: {finish_reason_val}")

            safety_ratings = getattr(candidate, 'safety_ratings', [])
            for rating in safety_ratings:
                 if getattr(rating, 'blocked', False):
                     block_reason = f"Candidate blocked (Category: {getattr(rating, 'category', 'N/A').name})"
                     print(f"âŒ Gemini {block_reason}")
                     error_message = f"ì˜¤ë¥˜ ë°œìƒ: ìƒì„±ëœ ë‚´ìš©ì´ ì•ˆì „ ì •ì±…ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (ì‚¬ìœ : {block_reason})."
                     return error_message, token_info # Return error early

            # Extract text content
            try:
                if candidate.content and candidate.content.parts:
                    generated_text = "".join(part.text for part in candidate.content.parts if hasattr(part, 'text'))
                # Sometimes the text might be directly in candidate.text
                elif hasattr(candidate, 'text') and isinstance(candidate.text, str):
                     generated_text = candidate.text

            except (AttributeError, IndexError, TypeError) as text_extract_err:
                print(f"âš ï¸ Gemini response text extraction error: {text_extract_err}")
                generated_text = None

        # Return logic
        if generated_text is not None:
            if finish_reason_val not in ['STOP', '1', 'FINISH_REASON_STOP']:
                print(f"âš ï¸ Gemini generation finished with non-STOP reason: {finish_reason_val}")
                # Append warning only if reason indicates potential truncation/issue
                if finish_reason_val in ['MAX_TOKENS', 'SAFETY', 'RECITATION', 'OTHER']:
                     generated_text += f"\n\n[!] ìƒì„± ì¤‘ë‹¨ë¨ (ì‚¬ìœ : {finish_reason_val})"
            return generated_text, token_info
        else:
            if block_reason: # If already blocked, return that message
                 error_message = f"ì˜¤ë¥˜ ë°œìƒ: ì…ë ¥ ë˜ëŠ” ìƒì„± ë‚´ìš© ì°¨ë‹¨ë¨ (ì‚¬ìœ : {block_reason})."
            else: # No text and not explicitly blocked
                 error_message = f"ì˜¤ë¥˜ ë°œìƒ: API ì‘ë‹µì—ì„œ ìƒì„±ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¢…ë£Œ ì‚¬ìœ : {finish_reason_val})."
                 print(f"âŒ {error_message}. Full Response: {response}")
            return error_message, token_info

    except google.api_core.exceptions.GoogleAPIError as e:
        # ... (keep existing detailed Gemini error handling) ...
        error_message = f"ì˜¤ë¥˜: Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e.__class__.__name__}: {e}"
        print(f"âŒ API HANDLER (Gemini): {error_message}")
        # Detailed error messages based on exception type
        if isinstance(e, google.api_core.exceptions.InvalidArgument): error_message = f"ì˜¤ë¥˜: ì˜ëª»ëœ ìš”ì²­ ì¸ìˆ˜ (ëª¨ë¸ëª…, APIí‚¤ ë“± í™•ì¸).\n{e}"
        elif isinstance(e, google.api_core.exceptions.ResourceExhausted): error_message = f"ì˜¤ë¥˜: API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” ë¦¬ì†ŒìŠ¤ ë¶€ì¡±.\n{e}"
        elif isinstance(e, google.api_core.exceptions.DeadlineExceeded): error_message = f"ì˜¤ë¥˜: API ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (í˜„ì¬ {request_timeout}ì´ˆ).\n{e}"
        elif isinstance(e, google.api_core.exceptions.PermissionDenied): error_message = f"ì˜¤ë¥˜: API í‚¤ ê¶Œí•œ ë¶€ì¡± ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŒ.\n{e}"
        elif hasattr(e, 'message') and "API key not valid" in e.message: error_message = f"ì˜¤ë¥˜: Gemini API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n{e}"
        return error_message, token_info # Return error and token_info (might be 0)
    except Exception as e:
        error_message = f"ì˜¤ë¥˜: Gemini ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œ ë°œìƒ: {e.__class__.__name__}: {e}"
        print(f"âŒ API HANDLER (Gemini): {error_message}")
        traceback.print_exc()
        return error_message, token_info # Return error and token_info (might be 0)


def _generate_with_claude(model_name, prompt, system_prompt, temperature):
    """Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
    print(f"API HANDLER (Claude): Calling model '{model_name}'...") # DEBUG
    token_info = {constants.INPUT_TOKEN_KEY: 0, constants.OUTPUT_TOKEN_KEY: 0}
    api_key = os.getenv(constants.ANTHROPIC_API_KEY_ENV)

    if not api_key: return "ì˜¤ë¥˜: Claude API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.", token_info
    if anthropic is None: return "ì˜¤ë¥˜: Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (`pip install anthropic`)", token_info

    try:
        client = anthropic.Anthropic(api_key=api_key) # Client can be created here
        try:
            temp_value = float(temperature); temp_value = max(0.0, min(1.0, temp_value)) # Claude temp range: 0.0 to 1.0
        except (ValueError, TypeError): temp_value = constants.DEFAULT_TEMPERATURE # Use default if invalid

        # Define max_tokens (important for Claude)
        # Consider making this dynamic based on 'length_option' if passed, or use a generous default
        max_tokens_to_generate = 4096 # Claude 3.5 default max output is high

        print(f"ğŸ¤– API HANDLER (Claude): Calling model '{model_name}' (Temp: {temp_value:.2f}, MaxTokens: {max_tokens_to_generate})...")
        start_time = time.time()

        # Construct the message list
        messages = [{"role": "user", "content": prompt}]
        # Claude uses 'system' parameter for system prompt
        system_param = system_prompt if system_prompt and system_prompt.strip() else None

        response = client.messages.create(
            model=model_name,
            max_tokens=max_tokens_to_generate,
            temperature=temp_value,
            system=system_param,
            messages=messages
        )

        end_time = time.time()
        print(f"âœ… API HANDLER (Claude): Response received ({end_time - start_time:.2f}s)")

        # --- Claude Token Extraction ---
        if hasattr(response, 'usage'):
            token_info = {
                constants.INPUT_TOKEN_KEY: getattr(response.usage, 'input_tokens', 0),
                constants.OUTPUT_TOKEN_KEY: getattr(response.usage, 'output_tokens', 0)
            }
            print(f"ğŸ“Š Claude Tokens: Input={token_info[constants.INPUT_TOKEN_KEY]}, Output={token_info[constants.OUTPUT_TOKEN_KEY]}")
        else:
             print("âš ï¸ Claude Tokens: No usage info found in response.")

        # --- Claude Response Handling ---
        generated_text = ""
        finish_reason = "UNKNOWN"
        if hasattr(response, 'content') and response.content:
            # Content is a list, usually with one text block
            for block in response.content:
                if getattr(block, 'type', '') == 'text':
                    generated_text += getattr(block, 'text', '')
        if hasattr(response, 'stop_reason'):
            finish_reason = response.stop_reason
            print(f"â„¹ï¸ Claude Finish Reason: {finish_reason}")

        if generated_text:
            if finish_reason not in ['end_turn', 'stop_sequence']: # Not a normal completion
                print(f"âš ï¸ Claude generation finished with non-standard reason: {finish_reason}")
                # Append warning if truncated
                if finish_reason == 'max_tokens':
                     generated_text += f"\n\n[!] ìƒì„± ì¤‘ë‹¨ë¨ (ì‚¬ìœ : ìµœëŒ€ í† í° ë„ë‹¬)"
            return generated_text, token_info
        else:
            error_message = f"ì˜¤ë¥˜ ë°œìƒ: Claude API ì‘ë‹µì—ì„œ ìƒì„±ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¢…ë£Œ ì‚¬ìœ : {finish_reason})."
            print(f"âŒ {error_message}. Full Response: {response}")
            return error_message, token_info # Return error and token_info

    except anthropic.APIError as e:
        # ... (keep existing detailed Claude error handling) ...
        error_message = f"ì˜¤ë¥˜: Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e.__class__.__name__}: {e}"
        print(f"âŒ API HANDLER (Claude): {error_message}")
        if isinstance(e, anthropic.AuthenticationError): error_message = f"ì˜¤ë¥˜: Claude API ì¸ì¦ ì‹¤íŒ¨ (API í‚¤ í™•ì¸).\n{e}"
        elif isinstance(e, anthropic.PermissionDeniedError): error_message = f"ì˜¤ë¥˜: Claude API ê¶Œí•œ ë¶€ì¡±.\n{e}"
        elif isinstance(e, anthropic.RateLimitError): error_message = f"ì˜¤ë¥˜: Claude API í˜¸ì¶œ ì œí•œ ì´ˆê³¼.\n{e}"
        elif isinstance(e, anthropic.NotFoundError) and hasattr(e, 'message') and 'model' in e.message: error_message = f"ì˜¤ë¥˜: Claude ëª¨ë¸ '{model_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n{e}"
        elif isinstance(e, anthropic.BadRequestError) and hasattr(e, 'message') and 'invalid system prompt' in e.message: error_message = f"ì˜¤ë¥˜: Claude ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.\n{e}"
        return error_message, token_info # Return error and token_info
    except Exception as e:
        error_message = f"ì˜¤ë¥˜: Claude ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œ ë°œìƒ: {e.__class__.__name__}: {e}"
        print(f"âŒ API HANDLER (Claude): {error_message}")
        traceback.print_exc()
        return error_message, token_info # Return error and token_info


def _generate_with_gpt(model_name, prompt, system_prompt, temperature):
    """OpenAI GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
    print(f"API HANDLER (GPT): Calling model '{model_name}'...") # DEBUG
    token_info = {constants.INPUT_TOKEN_KEY: 0, constants.OUTPUT_TOKEN_KEY: 0}
    api_key = os.getenv(constants.OPENAI_API_KEY_ENV)

    if not api_key: return "ì˜¤ë¥˜: OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.", token_info
    if openai is None: return "ì˜¤ë¥˜: OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (`pip install openai`)", token_info

    try:
        client = openai.OpenAI(api_key=api_key) # Client created here
        try:
            temp_value = float(temperature); temp_value = max(0.0, min(2.0, temp_value)) # OpenAI temp range: 0.0 to 2.0
        except (ValueError, TypeError): temp_value = constants.DEFAULT_TEMPERATURE

        # Construct message list including system prompt
        messages = []
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        # Consider adding max_tokens if needed, otherwise relies on model defaults
        # max_tokens_to_generate = 4096 # Example if needed

        print(f"ğŸ¤– API HANDLER (GPT): Calling model '{model_name}' (Temp: {temp_value:.2f})...")
        start_time = time.time()

        response = client.chat.completions.create(
            model=model_name,
            temperature=temp_value,
            messages=messages
            # max_tokens=max_tokens_to_generate # Uncomment if needed
        )

        end_time = time.time()
        print(f"âœ… API HANDLER (GPT): Response received ({end_time - start_time:.2f}s)")

        # --- GPT Token Extraction ---
        if hasattr(response, 'usage'):
            token_info = {
                constants.INPUT_TOKEN_KEY: getattr(response.usage, 'prompt_tokens', 0),
                constants.OUTPUT_TOKEN_KEY: getattr(response.usage, 'completion_tokens', 0) # OpenAI uses completion_tokens
            }
            print(f"ğŸ“Š GPT Tokens: Input={token_info[constants.INPUT_TOKEN_KEY]}, Output={token_info[constants.OUTPUT_TOKEN_KEY]}")
        else:
             print("âš ï¸ GPT Tokens: No usage info found in response.")

        # --- GPT Response Handling ---
        generated_text = None
        finish_reason = "UNKNOWN"
        if response.choices:
             choice = response.choices[0]
             if choice.message:
                 generated_text = choice.message.content
             finish_reason = choice.finish_reason
             print(f"â„¹ï¸ GPT Finish Reason: {finish_reason}")

        if generated_text is not None: # Check for None, empty string is valid
            if finish_reason != 'stop': # Not a normal completion
                print(f"âš ï¸ GPT generation finished with non-stop reason: {finish_reason}")
                # Append warning if truncated by length
                if finish_reason == 'length':
                     generated_text += f"\n\n[!] ìƒì„± ì¤‘ë‹¨ë¨ (ì‚¬ìœ : ìµœëŒ€ ê¸¸ì´ ë„ë‹¬)"
                elif finish_reason == 'content_filter':
                     # GPT usually errors out for content filter, but check anyway
                     generated_text += f"\n\n[!] ìƒì„± ì¤‘ë‹¨ë¨ (ì‚¬ìœ : ì½˜í…ì¸  í•„í„°)"
            return generated_text, token_info
        else:
            # Check for content filter block in the response structure if available
            # (This might be in response.prompt_annotations or similar, depends on API version)
            # Example placeholder:
            # if finish_reason == 'content_filter' or _check_gpt_content_filter(response):
            #    error_message = "ì˜¤ë¥˜ ë°œìƒ: ì…ë ¥ ë˜ëŠ” ìƒì„± ë‚´ìš©ì´ ì½˜í…ì¸  í•„í„°ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
            # else:
            error_message = f"ì˜¤ë¥˜ ë°œìƒ: GPT API ì‘ë‹µì—ì„œ ìƒì„±ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¢…ë£Œ ì‚¬ìœ : {finish_reason})."
            print(f"âŒ {error_message}. Full Response: {response}")
            return error_message, token_info # Return error and token_info

    except openai.APIError as e:
        # ... (keep existing detailed GPT error handling) ...
        error_message = f"ì˜¤ë¥˜: GPT API í˜¸ì¶œ ì‹¤íŒ¨: {e.__class__.__name__}: {e}"
        print(f"âŒ API HANDLER (GPT): {error_message}")
        if isinstance(e, openai.AuthenticationError): error_message = f"ì˜¤ë¥˜: GPT API ì¸ì¦ ì‹¤íŒ¨ (API í‚¤ í™•ì¸).\n{e}"
        elif isinstance(e, openai.PermissionDeniedError): error_message = f"ì˜¤ë¥˜: GPT API ê¶Œí•œ ë¶€ì¡±.\n{e}"
        elif isinstance(e, openai.RateLimitError): error_message = f"ì˜¤ë¥˜: GPT API í˜¸ì¶œ ì œí•œ ì´ˆê³¼.\n{e}"
        elif isinstance(e, openai.NotFoundError) and hasattr(e, 'message') and 'model' in e.message: error_message = f"ì˜¤ë¥˜: GPT ëª¨ë¸ '{model_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n{e}"
        elif isinstance(e, openai.BadRequestError): error_message = f"ì˜¤ë¥˜: GPT ìš”ì²­ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤ (í”„ë¡¬í”„íŠ¸ í™•ì¸).\n{e}"
        elif isinstance(e, openai.APIConnectionError): error_message = f"ì˜¤ë¥˜: GPT API ì—°ê²° ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ í™•ì¸).\n{e}"
        # Check for content policy violation specifically if possible
        if hasattr(e, 'code') and e.code == 'content_policy_violation':
             error_message = f"ì˜¤ë¥˜: GPT ì½˜í…ì¸  ì •ì±… ìœ„ë°˜ìœ¼ë¡œ ìš”ì²­ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n{e}"

        return error_message, token_info # Return error and token_info
    except Exception as e:
        error_message = f"ì˜¤ë¥˜: GPT ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œ ë°œìƒ: {e.__class__.__name__}: {e}"
        print(f"âŒ API HANDLER (GPT): {error_message}")
        traceback.print_exc()
        return error_message, token_info # Return error and token_info

# í˜¸í™˜ì„±ì„ ìœ„í•´ ì´ì „ í•¨ìˆ˜ëª…ë„ ìœ ì§€ (Gemini í˜¸ì¶œë¡œ ì—°ê²°)
def generate_webnovel_api_call(model_name, prompt, system_prompt, temperature=constants.DEFAULT_TEMPERATURE):
     print("WARN: generate_webnovel_api_call() is deprecated. Use generate_webnovel_scene_api_call() with API type.")
     return _generate_with_gemini(model_name, prompt, system_prompt, temperature)